<?xml version="1.0" encoding="utf-8" standalone="yes" ?>
<rss version="2.0" xmlns:atom="http://www.w3.org/2005/Atom">
  <channel>
    <title>Blogs on dremio | reimagining data analytics for the modern world</title>
    <link>localhost:1313</link>
    <description>Recent content in Blogs on dremio | reimagining data analytics for the modern world</description>
    <generator>Hugo -- gohugo.io</generator>
    <language>en-us</language>
    <lastBuildDate>Mon, 20 Mar 2017 00:00:00 +0000</lastBuildDate>
    <atom:link href="#ZgotmplZ" rel="self" type="application/rss+xml" />
    
    <item>
      <title>What Is A Data Warehouse?</title>
      <link>localhost:1313</link>
      <pubDate>Mon, 20 Mar 2017 00:00:00 +0000</pubDate>
      
      <guid>localhost:1313</guid>
      <description>

&lt;h2 id=&#34;what-is-a-data-warehouse&#34;&gt;What is a Data Warehouse?&lt;/h2&gt;

&lt;p&gt;For over 30 years, we&amp;rsquo;ve approached data analytics the same way - build another data warehouse. Today there are new options with better tradeoffs.&lt;/p&gt;

&lt;p&gt;We&amp;rsquo;ve published a new page - &lt;a href=&#34;http://www.dremio.com/what-is-data-warehouse&#34;&gt;What Is A Data Warehouse?&lt;/a&gt; - as a survey of the market today. This is our best attempt at being unbiased. We hope you find it useful!&lt;/p&gt;
</description>
    </item>
    
    <item>
      <title>ETL Tools Explained</title>
      <link>localhost:1313</link>
      <pubDate>Wed, 15 Mar 2017 00:00:00 +0000</pubDate>
      
      <guid>localhost:1313</guid>
      <description>

&lt;h2 id=&#34;what-is-etl&#34;&gt;What is ETL?&lt;/h2&gt;

&lt;p&gt;The ETL model has been in use for over 30 years - read data from different sources, apply transformations, then save the results in a different system for analytics. Modern hardware and distributed processing create new models for accessing data for analytics.&lt;/p&gt;

&lt;p&gt;We put together a &lt;a href=&#34;http://www.dremio.com/etl-tools-explained&#34;&gt;survey of current ETL tools in the market&lt;/a&gt; to help users understand their options in terms of commercial products, open source projects, and cloud services. We hope you find it useful!&lt;/p&gt;
</description>
    </item>
    
    <item>
      <title>What Is Data Engineering?</title>
      <link>localhost:1313</link>
      <pubDate>Wed, 08 Mar 2017 00:00:00 +0000</pubDate>
      
      <guid>localhost:1313</guid>
      <description>

&lt;h2 id=&#34;what-is-data-engineering&#34;&gt;What is Data Engineering?&lt;/h2&gt;

&lt;p&gt;There&amp;rsquo;s a relatively new role in many companies called Data Engineering. This team is responsible for making it easier for analysts, data scientists, and systems to access and analyze data. Because the role is new, there are lots of questions about what this team does, what tools they use, and how they work with data.&lt;/p&gt;

&lt;p&gt;We&amp;rsquo;ve published a new page - &lt;a href=&#34;http://www.dremio.com/what-is-data-engineering&#34;&gt;What Is Data Engineering?&lt;/a&gt; - to help explain the topic. We hope you find it useful!&lt;/p&gt;
</description>
    </item>
    
    <item>
      <title>BI on Big Data: What are your options?</title>
      <link>localhost:1313</link>
      <pubDate>Wed, 08 Jun 2016 00:00:00 +0000</pubDate>
      
      <guid>localhost:1313</guid>
      <description>&lt;p&gt;Deciding what combination of technologies will yield the best &amp;lsquo;BI on Big Data&amp;rsquo; experience can be a major challenge for
data professionals. This presentation, given by Dremio CEO Tomer Shiran at Strata + Hadoop World London, aims to shed
some light on some of the solutions that are available in the space.&lt;/p&gt;

&lt;p&gt;In this session, three general classes &amp;lsquo;BI on Big Data&amp;rsquo; solutions were investigated:&lt;/p&gt;

&lt;ul&gt;
&lt;li&gt;ETL to Data Warehouse (custom scripts, Informatica, Talend, Pentaho)&lt;/li&gt;
&lt;li&gt;Monolithic All-in-one Solutions (Datameer, Platfora, Zoomdata)&lt;/li&gt;
&lt;li&gt;SQL-on-Big-Data (Apache Drill, Apache Impala, Presto, Hive, Spark, Kylin, AtScale)&lt;/li&gt;
&lt;/ul&gt;

&lt;p&gt;The following slides contain architectural descriptions, pros and cons, and a needs-based heuristic to assist those
trying to settle on a BI on Big Data solution.&lt;/p&gt;

&lt;p&gt;&lt;br&gt;&lt;/p&gt;

&lt;script async class=&#34;speakerdeck-embed&#34; data-id=&#34;eba9639391314e049d62c33acfbe9323&#34; data-ratio=&#34;1.77777777777778&#34; src=&#34;//speakerdeck.com/assets/embed.js&#34;&gt;&lt;/script&gt;
</description>
    </item>
    
    <item>
      <title>Introducing Apache Arrow: Columnar In-Memory Analytics</title>
      <link>localhost:1313</link>
      <pubDate>Wed, 17 Feb 2016 16:34:36 -0800</pubDate>
      
      <guid>localhost:1313</guid>
      <description>

&lt;p&gt;Apache Arrow establishes a de-facto standard for columnar in-memory analytics which will redefine the performance and
interoperability of most Big Data technologies. The lead developers of 13 major open source Big Data projects have
joined forces to create Arrow, and additional companies and projects are expected to adopt and leverage the technology
in the coming months. Within the next few years, I expect the vast majority of all new data in the world to move through
Arrow’s columnar in-memory layer.&lt;/p&gt;

&lt;h3 id=&#34;columnar-in-memory-complex-analytics&#34;&gt;Columnar in-memory complex analytics&lt;/h3&gt;

&lt;p&gt;Arrow grew out of three prevailing trends and business requirements:&lt;/p&gt;

&lt;ul&gt;
&lt;li&gt;&lt;strong&gt;Columnar&lt;/strong&gt;: Big Data has gone columnar. Led by the creation and adoption of Apache Parquet and other columnar data storage
technologies, the industry has experienced rapid adoption of columnar storage for analytical workloads. These formats
reduce the footprint and increase the performance of such workloads.&lt;/li&gt;
&lt;li&gt;&lt;strong&gt;In-memory&lt;/strong&gt;: In-memory systems like SAP HANA and Spark accelerate analytical workloads by holding data in memory. In
short, people are no longer willing to wait for non-in-memory systems.&lt;/li&gt;
&lt;li&gt;&lt;strong&gt;Complex data and dynamic schemas&lt;/strong&gt;: Real-world objects are easier to represent as hierarchical and nested data structures.
This has lead to the rise of formats/technologies such as JSON and document databases. Analytical systems must treat
this type of data as first class.&lt;/li&gt;
&lt;/ul&gt;

&lt;p&gt;There are many systems that support only one of the three requirements, and there are a few that support two of the
three requirements. However, no existing system supports all three. The Big Data community has now come together to
change this reality by creating the Apache Arrow project.&lt;/p&gt;

&lt;p&gt;Arrow combines the benefits of columnar data structures with in-memory computing. It provides the performance benefits
of these modern techniques while also providing the flexibility of complex data and dynamic schemas. And it does all of
this in an open source and standardized way.&lt;/p&gt;

&lt;h3 id=&#34;arrow-is-a-combination-of-data-structures-algorithms-and-cross-language-bindings&#34;&gt;Arrow is a combination of data structures, algorithms and cross-language bindings&lt;/h3&gt;

&lt;p&gt;Arrow consists of a number of connected technologies designed to be integrated into storage and execution engines. The
key components of Arrow include:&lt;/p&gt;

&lt;ol&gt;
&lt;li&gt;A set of defined data types, including both SQL and JSON types, such as Int, BigInt, Decimal, VarChar, Map, Struct and
Array&lt;/li&gt;
&lt;li&gt;Canonical columnar in-memory representations of data to support an arbitrarily complex record structure built on top of
the defined data types&lt;/li&gt;
&lt;li&gt;Common Arrow-aware companion data-structures including pick-lists, hash tables and queues&lt;/li&gt;
&lt;li&gt;IPC through shared memory, TCP/IP and RDMA&lt;/li&gt;
&lt;li&gt;Libraries for reading and writing columnar data in multiple languages including Java, C, C++ and Python&lt;/li&gt;
&lt;li&gt;Pipelined and SIMD algorithms for various operations including bitmap selection, hashing, filtering, bucketing, sorting
and matching&lt;/li&gt;
&lt;li&gt;Columnar in-memory compression techniques to increase memory efficiency&lt;/li&gt;
&lt;li&gt;Tools for short-term persistence to non-volatile memory, SSD or HDD&lt;/li&gt;
&lt;/ol&gt;

&lt;h3 id=&#34;arrow-is-not-a-storage-or-execution-engine&#34;&gt;Arrow is not a storage or execution engine&lt;/h3&gt;

&lt;p&gt;Note that Arrow itself is not a storage or execution engine. It is designed to serve as a shared foundation for the
following types of systems:&lt;/p&gt;

&lt;ul&gt;
&lt;li&gt;SQL execution engines (such as Drill and Impala)&lt;/li&gt;
&lt;li&gt;Data analysis systems (such as Pandas and Spark)&lt;/li&gt;
&lt;li&gt;Streaming and queueing systems (such as Herron, Kafka and Storm)&lt;/li&gt;
&lt;li&gt;Storage systems (such as Parquet, Kudu, Cassandra and HBase)&lt;/li&gt;
&lt;/ul&gt;

&lt;p&gt;As such, Arrow doesn’t compete with any of these projects. Its core goal is to work within each of these projects to
provide faster performance and better interoperability. In fact, Arrow is being built by the lead developers of these
projects.&lt;/p&gt;

&lt;h3 id=&#34;performance&#34;&gt;Performance&lt;/h3&gt;

&lt;p&gt;The faster a user can get to the answer, the faster they can ask other questions. Performance begets more analysis, more
use cases and further innovation. As CPUs become faster and more sophisticated, one of the key challenges is making sure
processing technology leverages CPUs efficiently.&lt;/p&gt;

&lt;p&gt;Columnar in-memory analytics is first and foremost about ensuring optimal use of modern CPUs. Arrow is specifically
designed to maximize cache locality, pipelining and SIMD instructions.&lt;/p&gt;

&lt;p&gt;Arrow memory buffers are compact representations of data designed for modern CPUs. The structures are defined linearly,
matching typical read patterns. That means that data of similar type is collocated in memory. This makes cache
prefetching more effective, minimizing CPU stalls resulting from cache misses and main memory accesses. These
CPU-efficient data structures and access patterns extend to both traditional flat relational structures and modern
complex data structures.&lt;/p&gt;

&lt;p&gt;From there, execution patterns are designed to take advantage of the superscalar and pipelined nature of modern
processors. This is done by minimizing in-loop instruction count and loop complexity. These tight loops lead to better
performance and less branch-prediction failures.&lt;/p&gt;

&lt;p&gt;Finally, the data structures themselves are designed for modern superword and SIMD instructions. Single Instruction
Multiple Data (SIMD) instructions allow execution algorithms to operate more efficiently by executing multiple
operations in a single clock cycle. In some cases, when using AVX instructions, these optimizations can increase
performance by two orders of magnitude.&lt;/p&gt;

&lt;p&gt;Cache locality, pipelining and superword operations frequently provide 10-100x faster execution performance. Since many
analytical workloads are CPU bound, these benefits translate into dramatic end-user performance gains. These gains
result in faster answers and higher levels of user concurrency.&lt;/p&gt;

&lt;p&gt;&lt;br&gt;
&lt;p style=&#34;text-align: center;&#34;&gt;
&lt;img style=&#34;max-width: 65%;&#34; src=&#34;localhost:1313/img/arrow1.png&#34;&gt;
&lt;/p&gt;&lt;/p&gt;

&lt;h3 id=&#34;memory-efficiency&#34;&gt;Memory efficiency&lt;/h3&gt;

&lt;p&gt;In-memory performance is great, but memory can be scarce. Arrow is designed to work even if the data doesn’t fit
entirely in memory! The core data structure includes vectors of data and collections of these vectors (also called
record batches). Record batches are typically 64KB-1MB, depending on the workload, and are always bounded at 2^16
records. This not only improves cache locality, but also makes in-memory computing possible even in low-memory
situations. Arrow uses micro-pointers to minimize memory overhead and retrieval costs where inter-data-structure
pointers exist.&lt;/p&gt;

&lt;p&gt;With many Big Data clusters reaching 100s to 1000s of servers, systems must be able to take advantage of the aggregate
memory of a cluster. Arrow is designed to minimize the cost of moving data on the network. It utilizes scatter/gather
reads and writes and features a zero-serialization/deserialization design, allowing low-cost data movement between
nodes. Arrow also works directly with RDMA-capable interconnects to provide a singular memory grid for larger in-memory
workloads.&lt;/p&gt;

&lt;h3 id=&#34;zero-overhead-data-sharing&#34;&gt;Zero-overhead data sharing&lt;/h3&gt;

&lt;p&gt;Arrow helps both execution engines and storage systems. It increases the internal performance of execution engines, and
also provides a canonical way to communicate analytical data efficiently across systems.&lt;/p&gt;

&lt;p&gt;Today there is no standard way for systems to transfer data. As a result, all communication involves serializing,
copying an deserializing data. The serialization often takes place in the RPC layer, but there may also be implicit
serialization imposed by the use of a shared set of record- and value-level APIs. In such cases, when working with
multiple systems—such as Apache Drill reading from Apache Kudu—substantial resources and time are spent converting from
one system’s internal representation to the other system’s internal representation.&lt;/p&gt;

&lt;p&gt;As Arrow is adopted as the internal representation in each system, systems have the same internal representation of
data. This means that one system can hand data directly to the other system for consumption. And when these systems are
collocated on the same node, the copy described above can also be avoided through the use of shared memory. This means
that in many cases, moving data between two systems will have zero overhead.&lt;/p&gt;

&lt;p&gt;&lt;br&gt;
&lt;p style=&#34;text-align: center;&#34;&gt;
&lt;img style=&#34;max-width: 100%;&#34; src=&#34;localhost:1313/img/arrow2.png&#34;&gt;
&lt;/p&gt;&lt;/p&gt;

&lt;h3 id=&#34;first-class-support-for-popular-programming-languages&#34;&gt;First-class support for popular programming languages&lt;/h3&gt;

&lt;p&gt;One of the key outcomes users will see, beyond performance and interoperability, is a level-playing field among
different programming languages. Traditional data sharing is based on IPC and API-level integrations. While this is
often simple, it results in poor performance if the user’s language is different from the underlying system’s language.
Depending on the language and the set of algorithms implemented, the required transformation often represent the
majority of the processing time consumed.&lt;/p&gt;

&lt;p&gt;This all changes with Arrow. By providing a canonical columnar in-memory representation of data, each programming
language can interact directly with the raw data. C, C++, Java and Python Arrow libraries are underway and we will see
integrations for R, Julia and JavaScript soon.&lt;/p&gt;

&lt;h3 id=&#34;open-source-based-on-a-shared-and-acute-need&#34;&gt;Open source based on a shared and acute need&lt;/h3&gt;

&lt;p&gt;Leading execution engines are continuously seeking opportunities to improve performance. Developers in the Drill, Impala
and Spark communities have all separately recognized the need for a columnar in-memory approach to data.&lt;/p&gt;

&lt;ul&gt;
&lt;li&gt;Drill developers have implemented a columnar in-memory analytics layer inside Drill known as ValueVectors. Drill
provides an execution layer that performs SQL processing directly on columnar data without row materialization. The
combination of optimizations for columnar storage and direct columnar execution significantly lowers memory footprints
and provides faster execution of BI and SQL workloads.&lt;/li&gt;
&lt;li&gt;Impala developers also forecasted the need for a columnar in-memory approach in their 2015 CIDR paper:
“We are also considering switching to a columnar canonical in-memory format for data that needs to be materialized
during query processing, in order to take advantage of SIMD instructions”
(&lt;a href=&#34;http://www.cidrdb.org/cidr2015/Papers/CIDR15_Paper28.pdf&#34;&gt;http://www.cidrdb.org/cidr2015/Papers/CIDR15_Paper28.pdf&lt;/a&gt;)&lt;/li&gt;
&lt;li&gt;Spark developers also recognized this need, speaking of similar goals when outlining their vision for the Spark Tungsten
initiative:
“We’ve found that a large fraction of the CPU time is spent waiting for data to be fetched from main memory. As part of
Project Tungsten, we are designing cache-friendly algorithms and data structures so Spark applications will spend less
time waiting to fetch data from memory and more time doing useful work”
(&lt;a href=&#34;https://databricks.com/blog/2015/04/28/project-tungsten-bringing-spark-closer-to-bare-metal.html&#34;&gt;https://databricks.com/blog/2015/04/28/project-tungsten-bringing-spark-closer-to-bare-metal.html&lt;/a&gt;)&lt;/li&gt;
&lt;/ul&gt;

&lt;p&gt;The Big Data community has recognized an opportunity to develop a shared technology to address columnar in-memory
analytics, and has joined forces to create Apache Arrow. The Apache Drill community is seeding the project with the Java
library, based on Drill’s existing ValueVectors technology, and Wes McKinney, creator of Pandas and Ibis, is
contributing the initial C/C++ library. Given the credentials of those involved as well as code provenance, the Apache
Software Foundation decided to make Apache Arrow a Top-Level Project, highlighting the importance of the project and
community behind it.&lt;/p&gt;

&lt;h3 id=&#34;a-diverse-community-of-open-source-leaders&#34;&gt;A diverse community of open source leaders&lt;/h3&gt;

&lt;p&gt;Many lead developers (PMC Chairs, PMC Members, Committers, etc.) of &lt;strong&gt;13&lt;/strong&gt; major open source projects are involved in Apache
Arrow:&lt;/p&gt;

&lt;ul&gt;
&lt;li&gt;Calcite&lt;/li&gt;
&lt;li&gt;Cassandra&lt;/li&gt;
&lt;li&gt;Drill&lt;/li&gt;
&lt;li&gt;Hadoop&lt;/li&gt;
&lt;li&gt;HBase&lt;/li&gt;
&lt;li&gt;Ibis&lt;/li&gt;
&lt;li&gt;Impala&lt;/li&gt;
&lt;li&gt;Kudu&lt;/li&gt;
&lt;li&gt;Pandas&lt;/li&gt;
&lt;li&gt;Parquet&lt;/li&gt;
&lt;li&gt;Phoenix&lt;/li&gt;
&lt;li&gt;Spark&lt;/li&gt;
&lt;li&gt;Storm&lt;/li&gt;
&lt;/ul&gt;

&lt;h3 id=&#34;what-s-next&#34;&gt;What’s next?&lt;/h3&gt;

&lt;p&gt;The code for Apache Arrow is now available under the Apache 2.0 License for use in both open source and proprietary
systems. Arrow implementations and bindings are available or underway for C, C++, Java and Python.&lt;/p&gt;

&lt;p&gt;Drill, Impala, Kudu, Ibis and Spark will become Arrow-enabled this year, and I anticipate that many other projects will
embrace Arrow in the near future as well. Arrow community members (including myself) will be speaking at upcoming
conferences including Strata San Jose, Strata London and numerous meetups.&lt;/p&gt;

&lt;p&gt;Join the community today. Follow @ApacheArrow, connect on the dev@arrow.apache.org list or check out the code at
git://git.apache.org/arrow.git&lt;/p&gt;
</description>
    </item>
    
    <item>
      <title>Parsing EPA vehicle data for linear correlations in MPG ratings</title>
      <link>localhost:1313</link>
      <pubDate>Thu, 11 Feb 2016 00:00:00 +0000</pubDate>
      
      <guid>localhost:1313</guid>
      <description>&lt;p&gt;In &lt;a href=&#34;http://www.dremio.com/blog/calculating-pearsons-r-using-a-custom-sql-function/&#34;&gt;the previous day&amp;rsquo;s post&lt;/a&gt; I
demonstrated how to code a custom aggregate function that can process two sets of data points into their corresponding
Pearson&amp;rsquo;s &lt;em&gt;r&lt;/em&gt; value, which is a useful indicator of variable correlation. Today I&amp;rsquo;m going to put that function to the
test on this &lt;a href=&#34;https://www.fueleconomy.gov/feg/download.shtml&#34;&gt;EPA data set&lt;/a&gt; that contains information about vehicles
manufactured from model years 1984 to 2017. The two questions I&amp;rsquo;d like to answer are: 1.) Does the combined MPG rating
for a car correlate with model year? and 2.) How does an engine&amp;rsquo;s displacement affect the combined MPG rating?&lt;/p&gt;

&lt;p&gt;To begin with, I&amp;rsquo;m going to create two views that average over combined MPG for each of the other variables of interest.
These are constructed as follows:&lt;/p&gt;

&lt;pre&gt;&lt;code&gt;CREATE VIEW year_mpg AS
     SELECT CAST(`year` AS FLOAT) `year`, AVG(CAST(comb08 AS FLOAT)) mpg
       FROM dfs.`/Users/ngriffith/Downloads/vehicles.csvh`
   GROUP BY `year`
   ORDER BY `year`;
&lt;/code&gt;&lt;/pre&gt;

&lt;p&gt;helps answer the first question about model years, while&lt;/p&gt;

&lt;pre&gt;&lt;code&gt;CREATE VIEW displacement_mpg AS
     SELECT CAST(displ AS FLOAT) displacement, AVG(CAST(comb08 AS FLOAT)) mpg
       FROM dfs.`/Users/ngriffith/Downloads/vehicles.csvh`
      WHERE displ NOT LIKE &#39;&#39;
        AND displ NOT LIKE &#39;NA&#39;
   GROUP BY displ
   ORDER BY displacement;
&lt;/code&gt;&lt;/pre&gt;

&lt;p&gt;will allow us tackle the second one about engine sizes.&lt;/p&gt;

&lt;p&gt;The first view looks like this:&lt;/p&gt;

&lt;pre&gt;&lt;code&gt;&amp;gt; SELECT * FROM year_mpg;
+---------+---------------------+
|  year   |         mpg         |
+---------+---------------------+
| 1984.0  | 19.881873727087576  |
| 1985.0  | 19.808348030570254  |
| 1986.0  | 19.550413223140495  |
| 1987.0  | 19.228548516439453  |
| 1988.0  | 19.328318584070797  |
| 1989.0  | 19.12575888985256   |
| 1990.0  | 19.000927643784788  |
| 1991.0  | 18.825971731448764  |
| 1992.0  | 18.86262265834077   |
| 1993.0  | 19.104300091491307  |
| 1994.0  | 19.0122199592668    |
| 1995.0  | 18.797311271975182  |
| 1996.0  | 19.584734799482536  |
| 1997.0  | 19.429133858267715  |
| 1998.0  | 19.51847290640394   |
| 1999.0  | 19.61150234741784   |
| 2000.0  | 19.526190476190475  |
| 2001.0  | 19.479692645444565  |
| 2002.0  | 19.168205128205127  |
| 2003.0  | 19.00095785440613   |
| 2004.0  | 19.067736185383243  |
| 2005.0  | 19.193825042881645  |
| 2006.0  | 18.95923913043478   |
| 2007.0  | 18.97868561278863   |
| 2008.0  | 19.27632687447346   |
| 2009.0  | 19.74070945945946   |
| 2010.0  | 20.601442741208295  |
| 2011.0  | 21.10353982300885   |
| 2012.0  | 21.93755420641804   |
| 2013.0  | 23.253164556962027  |
| 2014.0  | 23.70114006514658   |
| 2015.0  | 24.214953271028037  |
| 2016.0  | 24.84784446322908   |
| 2017.0  | 23.571428571428573  |
+---------+---------------------+
&lt;/code&gt;&lt;/pre&gt;

&lt;p&gt;Yup, it definitely looks like there&amp;rsquo;s a trend toward higher MPG. But let&amp;rsquo;s calculate the &lt;em&gt;r&lt;/em&gt; value:&lt;/p&gt;

&lt;pre&gt;&lt;code&gt;&amp;gt; SELECT PCORRELATION(`year`, mpg) FROM year_mpg;
Error: SYSTEM ERROR: SchemaChangeException: Failure while materializing expression.
Error in expression at index -1.  Error: Missing function implementation: [pcorrelation(FLOAT4-REQUIRED,
FLOAT8-OPTIONAL)].  Full expression: --UNKNOWN EXPRESSION--.
...
&lt;/code&gt;&lt;/pre&gt;

&lt;p&gt;Oops! We got an error message instead of an &lt;em&gt;r&lt;/em&gt; value. That&amp;rsquo;s because my &lt;code&gt;PCORRELATION()&lt;/code&gt; function expects two variables
that are nullable (&amp;lsquo;optional&amp;rsquo;), but the first one that&amp;rsquo;s getting passed is non-nullable (&amp;lsquo;required&amp;rsquo;). This situation is
what led to the creation of &lt;a href=&#34;http://www.dremio.com/blog/managing-variable-type-nullability/&#34;&gt;this earlier article and its associated
functions&lt;/a&gt; for stripping and adding nullability to
variables. The &lt;code&gt;ADD_NULL_FLOAT()&lt;/code&gt; custom function from that piece is exactly what we need to turn &lt;code&gt;`years `&lt;/code&gt; into a
variable type that &lt;code&gt;PCORRELATION()&lt;/code&gt; can accept.&lt;/p&gt;

&lt;p&gt;So let&amp;rsquo;s try this again:&lt;/p&gt;

&lt;pre&gt;&lt;code&gt;&amp;gt; SELECT PCORRELATION(ADD_NULL_FLOAT(`year`), mpg) FROM year_mpg;
+---------------------+
|       EXPR$0        |
+---------------------+
| 0.6870535033886027  |
+---------------------+
&lt;/code&gt;&lt;/pre&gt;

&lt;p&gt;Correlation confirmed! It&amp;rsquo;s not the strongest (that would be a value of 1.0), but it&amp;rsquo;s definitely there. Neat!&lt;/p&gt;

&lt;p&gt;Now to take a look at how engine size related. The view I created contains this data:&lt;/p&gt;

&lt;pre&gt;&lt;code&gt;&amp;gt; SELECT * FROM displacement_mpg;
+---------------+---------------------+
| displacement  |         mpg         |
+---------------+---------------------+
| 0.0           | 87.5                |
| 0.6           | 39.0                |
| 0.9           | 35.5                |
| 1.0           | 37.5030303030303    |
| 1.1           | 19.916666666666668  |
| 1.2           | 30.81081081081081   |
| 1.3           | 28.904255319148938  |
| 1.4           | 30.658959537572255  |
| 1.5           | 29.439592430858806  |
| 1.6           | 26.48841961852861   |
| 1.7           | 28.842105263157894  |
| 1.8           | 24.91231463571889   |
| 1.9           | 27.038277511961724  |
| 2.0           | 24.032035928143713  |
| 2.1           | 19.28301886792453   |
| 2.2           | 22.07820419985518   |
| 2.3           | 21.026008968609865  |
| 2.4           | 22.373468300479487  |
| 2.5           | 21.628227194492254  |
| 2.6           | 18.123529411764707  |
| 2.7           | 19.88589211618257   |
| 2.8           | 18.463019250253293  |
| 2.9           | 18.772727272727273  |
| 3.0           | 19.428571428571427  |
| 3.1           | 19.72156862745098   |
| 3.2           | 18.25237191650854   |
| 3.3           | 18.54698795180723   |
| 3.4           | 18.473317865429234  |
| 3.5           | 20.02212705210564   |
| 3.6           | 19.188457008244995  |
| 3.7           | 18.107468123861565  |
| 3.8           | 19.115025906735752  |
| 3.9           | 15.588815789473685  |
| 4.0           | 16.738241308793455  |
| 4.1           | 15.873684210526315  |
| 4.2           | 16.05597014925373   |
| 4.3           | 16.590775988286968  |
| 4.4           | 17.094736842105263  |
| 4.5           | 15.566666666666666  |
| 4.6           | 16.6696269982238    |
| 4.7           | 15.282548476454293  |
| 4.8           | 15.813688212927756  |
| 4.9           | 14.355113636363637  |
| 5.0           | 15.2375             |
| 5.2           | 13.063197026022305  |
| 5.3           | 15.203412073490814  |
| 5.4           | 13.902597402597403  |
| 5.5           | 15.16243654822335   |
| 5.6           | 14.0                |
| 5.6           | 14.394736842105264  |
| 5.7           | 14.780718336483933  |
| 5.8           | 11.78125            |
| 5.9           | 11.701183431952662  |
| 6.0           | 14.462686567164178  |
| 6.1           | 15.0                |
| 6.1           | 14.454545454545455  |
| 6.2           | 16.540930979133226  |
| 6.3           | 13.705882352941176  |
| 6.4           | 16.8                |
| 6.5           | 14.81081081081081   |
| 6.6           | 15.0                |
| 6.7           | 13.0                |
| 6.8           | 10.572463768115941  |
| 7.0           | 17.4                |
| 7.4           | 9.75                |
| 8.0           | 12.347826086956522  |
| 8.3           | 11.222222222222221  |
| 8.4           | 15.6                |
+---------------+---------------------+
&lt;/code&gt;&lt;/pre&gt;

&lt;p&gt;Which, after making a similar adjustment using &lt;code&gt;ADD_NULL_FLOAT()&lt;/code&gt;, yields a correlation of:&lt;/p&gt;

&lt;pre&gt;&lt;code&gt;&amp;gt; SELECT PCORRELATION(ADD_NULL_FLOAT(displacement), mpg) FROM displacement_mpg;
+---------------------+
|       EXPR$0        |
+---------------------+
| -0.679501632464044  |
+---------------------+
&lt;/code&gt;&lt;/pre&gt;

&lt;p&gt;So combined average combined MPG is almost as strongly anti-correlated with engine size as it is correlated with model
year. There&amp;rsquo;s a fairly clear linear dependence in both cases!&lt;/p&gt;

&lt;p&gt;For my next couple articles I&amp;rsquo;ll be digging even deeper into statistical techniques by implementing and testing a Drill
function to calculate the probability of events that follow a Poisson distribution. It&amp;rsquo;s shaping up to be a great
couple of weeks to be reading the Dremio Blog if you&amp;rsquo;re curious about what custom-tuned &amp;lsquo;SQL on anything&amp;rsquo; software can
do in terms of serious analysis.&lt;/p&gt;
</description>
    </item>
    
    <item>
      <title>What can LIGO see? Let&#39;s look at gravitational waves with SQL</title>
      <link>localhost:1313</link>
      <pubDate>Thu, 11 Feb 2016 00:00:00 +0000</pubDate>
      
      <guid>localhost:1313</guid>
      <description>&lt;script type=&#34;text/javascript&#34;
 src=&#34;http://cdn.mathjax.org/mathjax/latest/MathJax.js?config=TeX-AMS-MML_HTMLorMML&#34;&gt;
&lt;/script&gt;

&lt;p&gt;It&amp;rsquo;s difficult to overstate how thrilling today&amp;rsquo;s news about gravity waves is. The scientific community has been waiting
a &lt;em&gt;long&lt;/em&gt; time for this, and verification of the phenomenon has wide reaching implications in the fields of both
astrophysics and particle physics. Gravity is, after all, the biggest thorn in the side of modern theoretical particle
physics.&lt;/p&gt;

&lt;p&gt;From a particle-centric standpoint gravity wave detection is identical to &amp;lsquo;graviton&amp;rsquo; detection. This is important
because gravitons amount to the &amp;lsquo;missing link&amp;rsquo; between the currently disconnected realms of the very big (dictated by
Einstein&amp;rsquo;s general relativity) and the very small (governed by quantum mechanics). The observation of gravitational
waves may help to constrain the current multitude of competing quantum gravity theories, leading us closer what&amp;rsquo;s
frequently called the Holy Grail of physics: a Theory of Everything.&lt;/p&gt;

&lt;p&gt;Because the LIGO gravitational wave observatory is awesome, they&amp;rsquo;ve made some of the data relevant to their gravity wave
event public (go check out &lt;a href=&#34;https://losc.ligo.org/events/GW150914/&#34;&gt;this site&lt;/a&gt;). As you may have guessed, I&amp;rsquo;m going to
use &lt;a href=&#34;https://drill.apache.org/docs/drill-in-10-minutes/&#34;&gt;Apache Drill&lt;/a&gt; to say something about the data! In particular
I&amp;rsquo;ll be investigating just how sensitive their equipment is, and what else they may be able to detect.&lt;/p&gt;

&lt;p&gt;To do this analysis, I&amp;rsquo;ll be looking at the data for the &amp;lsquo;H1&amp;rsquo; LIGO detector shown in the leftmost plots of the top and
third rows of &lt;a href=&#34;https://losc.ligo.org/events/GW150914/&#34;&gt;Figure 1&lt;/a&gt;. These files are called &lt;code&gt;fig1-observed-H.txt&lt;/code&gt; and
&lt;code&gt;fig1-residual-H.txt&lt;/code&gt;, and they contain the signal-with-background and background-only time series data. To use these
with Drill, open them up and remove the first line of the file, which is a comment. Then go edit the &amp;lsquo;dfs&amp;rsquo; plugin JSON
(go to &lt;a href=&#34;http://localhost:8047/storage/dfs&#34;&gt;http://localhost:8047/storage/dfs&lt;/a&gt; after starting &lt;code&gt;drill-embdedded&lt;/code&gt;) so that you have an entry in &lt;code&gt;&amp;quot;formats&amp;quot;&lt;/code&gt; that
looks like:&lt;/p&gt;

&lt;pre&gt;&lt;code&gt;    &amp;quot;txt&amp;quot;: {
      &amp;quot;type&amp;quot;: &amp;quot;text&amp;quot;,
      &amp;quot;extensions&amp;quot;: [
        &amp;quot;txt&amp;quot;
      ],
      &amp;quot;delimiter&amp;quot;: &amp;quot; &amp;quot;
    },
&lt;/code&gt;&lt;/pre&gt;

&lt;p&gt;This ensures that Drill knows that to do with a file that ends in &amp;lsquo;.txt&amp;rsquo;, instructing it to treat spaces as the column
delimiter. With this done, let&amp;rsquo;s write a SQL query to find the standard deviation of the background noise:&lt;/p&gt;

&lt;pre&gt;&lt;code&gt;&amp;gt; SELECT STDDEV(CAST(columns[1] AS FLOAT)) FROM dfs.`/path/to/fig1-residual-H.txt`;
+----------------------+
|        EXPR$0        |
+----------------------+
| 0.16534411608717056  |
+----------------------+
&lt;/code&gt;&lt;/pre&gt;

&lt;p&gt;So now let&amp;rsquo;s ask: On average, how many standard deviations away from the noise are the 100 biggest signal data points?
First, we&amp;rsquo;ll make a view (remember &lt;code&gt;USE dfs.tmp;&lt;/code&gt;):&lt;/p&gt;

&lt;pre&gt;&lt;code&gt;CREATE VIEW bigsignal AS
     SELECT ABS(CAST(columns[1] AS FLOAT)/0.165) signal
       FROM dfs.`/Users/ngriffith/Downloads/LIGO/fig1-observed-H.txt`
   ORDER BY signal DESC
      LIMIT 100;
&lt;/code&gt;&lt;/pre&gt;

&lt;p&gt;and then we can take the average:&lt;/p&gt;

&lt;pre&gt;&lt;code&gt;&amp;gt; SELECT AVG(signal) FROM bigsignal;
+--------------------+
|       EXPR$0       |
+--------------------+
| 5.884780800703798  |
+--------------------+
&lt;/code&gt;&lt;/pre&gt;

&lt;p&gt;Not too far off from the event significance given in &lt;a href=&#34;https://dcc.ligo.org/P150914/public&#34;&gt;the paper&lt;/a&gt; of 5.3 sigma!
(Although their methodology is admittedly wildly different as well as much more rigorous and subtle.)&lt;/p&gt;

&lt;p&gt;But what exactly &lt;em&gt;is&lt;/em&gt; this signal? Well a LIGO detector works by precisely measuring two identical kilometer-scale
distances with lasers. The measured distances are are arranged in a cross-pattern, so that any passing gravitational
waves will cause these lengths to contract or expand relative to one another by different (&lt;em&gt;very tiny!&lt;/em&gt;) amounts. The
&amp;lsquo;signal&amp;rsquo; numbers that we&amp;rsquo;ve been looking at are the differences between these two lengths. One source of gravity waves
(and the one that the people who run LIGO indicate in their announcement) is two massive bodies, such as black holes,
orbiting one another.&lt;/p&gt;

&lt;p&gt;Now let&amp;rsquo;s have a little more fun. The length difference from the data should be directly proportional to the amplitude
\( A \) of the gravity wave, which in this situation expresses the proportionality (&lt;a href=&#34;https://en.wikipedia.org/wiki/Gravitational_wave#Power_radiated_by_orbiting_bodies&#34;&gt;according to
Wikipedia&lt;/a&gt;) of:&lt;/p&gt;

&lt;p&gt;$$ A \propto  \frac{m_1 m_2}{R} $$&lt;/p&gt;

&lt;p&gt;where \( m_1 \) and \( m_2 \) are the masses of the orbiting bodies, and \( R \) is the distance of the observer
from the center of mass of the two-body system.&lt;/p&gt;

&lt;p&gt;Decreasing the observed signal waveform of about 5.88 sigma by half would leave itself us comfortably near 3 sigma
territory, which is still a very strong indicator for significance (randomly achieving 3 sigma result is about a
one-in-ten-thousand event). Admittedly from a visual standpoint this wouldn&amp;rsquo;t leave a very strong looking signal, but
statistical analysis in conjunction with confirmation from another type of observatory (such as one looking at radio or
gamma rays) may yield useful astrophysical data.&lt;/p&gt;

&lt;p&gt;In the discovery paper the authors list two colliding black holes with similar masses (around 30 times the Sun) as a
likely source of the event that they observed. They also place the event at a distance of about 1.2 billion light-years
from Earth. If we can manage to notice gravity wave signals with half the strength, then LIGO would be able to detect
similar events twice as far away, or with 71% the constituent mass.&lt;/p&gt;

&lt;p&gt;The gravitational wave astronomy revolution has just started, and I&amp;rsquo;m extremely excited to see where it leads us!&lt;/p&gt;
</description>
    </item>
    
    <item>
      <title>Calculating Pearson&#39;s r using a custom SQL function</title>
      <link>localhost:1313</link>
      <pubDate>Wed, 10 Feb 2016 00:00:00 +0000</pubDate>
      
      <guid>localhost:1313</guid>
      <description>&lt;script type=&#34;text/javascript&#34;
 src=&#34;http://cdn.mathjax.org/mathjax/latest/MathJax.js?config=TeX-AMS-MML_HTMLorMML&#34;&gt;
&lt;/script&gt;

&lt;p&gt;Lately I&amp;rsquo;ve written a lot of custom functions to assist me in my example Drill analyses, but they&amp;rsquo;ve all been of the
same fundamental type: They take one or more columns of a single row and process them into a single output. The &lt;a href=&#34;https://drill.apache.org/docs/develop-custom-functions-introduction/&#34;&gt;Drill
documentation&lt;/a&gt; calls these &amp;ldquo;simple&amp;rdquo; functions.
However there&amp;rsquo;s another class of functions lurking out there&amp;mdash;ones that can accept &lt;em&gt;many&lt;/em&gt; rows of data as input. We
call them &amp;ldquo;aggregate&amp;rdquo; functions.&lt;/p&gt;

&lt;p&gt;If you&amp;rsquo;re an experienced user of SQL, you&amp;rsquo;re already familiar with a few very common aggregate functions like &lt;code&gt;COUNT()&lt;/code&gt;
and &lt;code&gt;SUM()&lt;/code&gt;, but you&amp;rsquo;ve probably never written one of your own. Today we&amp;rsquo;re going to change that!&lt;/p&gt;

&lt;p&gt;As I&amp;rsquo;ve discussed in previous articles Drill already has some built-in statistics functions, but the goal of this post
will be to expand those capabilities even further by implementing an aggregate function to calculate a value called
Pearson&amp;rsquo;s &lt;em&gt;r&lt;/em&gt;. Values for &lt;em&gt;r&lt;/em&gt; vary from +1 to -1, and indicate the degree to which two variables are linearly correlated
or anti-correlated, respectively. An &lt;em&gt;r&lt;/em&gt; value at or near 0 indicates that there is no linear relationship between the
two sets of data points.&lt;/p&gt;

&lt;p&gt;After looking on Wikipedia, the most Drill-friendly equation for Pearson&amp;rsquo;s &lt;em&gt;r&lt;/em&gt; is:&lt;/p&gt;

&lt;p&gt;$$ r = \frac{n \sum x_i y_i - \sum x_i \sum y_i}{ \sqrt{n \sum x_i^2 - \left( \sum x_i \right)^2} \sqrt{n \sum y_i^2 -
\left( \sum y_i \right)^2}} $$&lt;/p&gt;

&lt;p&gt;where \( x_i \) and \( y_i \) are our data points, and \( n \) is the total number of them.&lt;/p&gt;

&lt;p&gt;Once you&amp;rsquo;ve got a Maven project started for your Drill UDF (a guide is available in the &amp;ldquo;Downloading Maven and starting
a new project&amp;rdquo; section of &lt;a href=&#34;http://www.dremio.com/blog/writing-a-custom-sql-function-for-sentiment-analysis/&#34;&gt;this
article&lt;/a&gt;), take a look at the source
for our Pearson&amp;rsquo;s &lt;em&gt;r&lt;/em&gt; function:&lt;/p&gt;

&lt;pre&gt;&lt;code class=&#34;language-java&#34;&gt;package com.yourgroupidentifier.udf;

import org.apache.drill.exec.expr.DrillAggFunc;
import org.apache.drill.exec.expr.holders.IntHolder;
import org.apache.drill.exec.expr.holders.NullableFloat8Holder;
import org.apache.drill.exec.expr.holders.Float8Holder;

import org.apache.drill.exec.expr.annotations.FunctionTemplate;

import org.apache.drill.exec.expr.annotations.Param;
import org.apache.drill.exec.expr.annotations.Workspace;
import org.apache.drill.exec.expr.annotations.Output;

@FunctionTemplate(
        name = &amp;quot;pcorrelation&amp;quot;,
        scope = FunctionTemplate.FunctionScope.POINT_AGGREGATE,
        nulls = FunctionTemplate.NullHandling.INTERNAL
)

public class PCorrelation implements DrillAggFunc {

    @Param
    NullableFloat8Holder xInput;

    @Param
    NullableFloat8Holder yInput;

    @Workspace
    IntHolder numValues;

    @Workspace
    Float8Holder xSum;

    @Workspace
    Float8Holder ySum;

    @Workspace
    Float8Holder xSqSum;

    @Workspace
    Float8Holder ySqSum;

    @Workspace
    Float8Holder xySum;

    @Output
    Float8Holder output;

    public void setup() {
        // Initialize values
        numValues.value = 0;
        xSum.value = 0;
        ySum.value = 0;
        xSqSum.value = 0;
        ySqSum.value = 0;
        xySum.value = 0;
    }

    public void reset() {
        // Initialize values
        numValues.value = 0;
        xSum.value = 0;
        ySum.value = 0;
        xSqSum.value = 0;
        ySqSum.value = 0;
        xySum.value = 0;
    }

    public void add() {

        // Only proceed if both floats aren&#39;t nulls
        if( (xInput.isSet == 1) || (yInput.isSet == 1) ) {

            numValues.value++;

            xSum.value += xInput.value;
            ySum.value += yInput.value;

            xSqSum.value += xInput.value * xInput.value;
            ySqSum.value += yInput.value * yInput.value;

            xySum.value += xInput.value * yInput.value;
        }

    }

    public void output() {

        float n = numValues.value;

        double x = xSum.value;
        double y = ySum.value;

        double x2 = xSqSum.value;
        double y2 = ySqSum.value;

        double xy = xySum.value;

        output.value = (n*xy - x*y)/(Math.sqrt(n*x2 - x*x)*Math.sqrt(n*y2 - y*y));
    }
}
&lt;/code&gt;&lt;/pre&gt;

&lt;p&gt;Yes, that&amp;rsquo;s a chunk of code&amp;mdash;but it&amp;rsquo;s mostly this long because it takes a lot of variables to accomplish the &lt;em&gt;r&lt;/em&gt;
calculation. Anyway, let&amp;rsquo;s talk about some differences between this aggregate function and the simple ones we&amp;rsquo;ve been
writing up until now.&lt;/p&gt;

&lt;p&gt;First, up in the function template the scope changes from &lt;code&gt;SIMPLE&lt;/code&gt; to &lt;code&gt;POINT_AGGREGATE&lt;/code&gt;, while the null handling is set
to &lt;code&gt;INTERNAL&lt;/code&gt; instead of &lt;code&gt;NULL_IF_NULL&lt;/code&gt;. This is because aggregate functions need to determine on their own how to
process null inputs, rather than let Drill handle it for them as we can do for most simple functions. You&amp;rsquo;ll also notice
a new annotation, &lt;code&gt;@Workspace&lt;/code&gt;, which is used before variables that assist in the calculation of the result as the
function moves through each row.&lt;/p&gt;

&lt;p&gt;Another obvious difference is that aggregate functions implement a different set of methods than simple ones. The
&lt;code&gt;setup()&lt;/code&gt; method remains the same, but &lt;code&gt;output()&lt;/code&gt; takes the place of &lt;code&gt;eval()&lt;/code&gt;. For each row that&amp;rsquo;s processed &lt;code&gt;add()&lt;/code&gt; is
called, and &lt;code&gt;reset()&lt;/code&gt; is used to determine what the function does when it hits a new set of rows.&lt;/p&gt;

&lt;p&gt;In the next article, I&amp;rsquo;ll take this new &lt;code&gt;PCORRELATION()&lt;/code&gt; function out for a spin on some vehicle data from the EPA.&lt;/p&gt;

&lt;p&gt;(OK, yes, pun very much intended that time.)&lt;/p&gt;
</description>
    </item>
    
    <item>
      <title>Managing variable type nullability</title>
      <link>localhost:1313</link>
      <pubDate>Tue, 09 Feb 2016 00:00:00 +0000</pubDate>
      
      <guid>localhost:1313</guid>
      <description>&lt;p&gt;One day you may find yourself with a custom function for Drill that&amp;rsquo;s very particular about the kind of variables that
it accepts. In particular, it may hold strong opinions about whether or not a variable is allowed to express a &lt;code&gt;NULL&lt;/code&gt;
value. In fact it may even be &lt;em&gt;you&lt;/em&gt; who wrote this unavoidably fussy function (SPOILER: This is exactly what happened to
me earlier this week).&lt;/p&gt;

&lt;p&gt;Currently Drill lacks built-in functions to add or strip nullability from variables, but luckily it&amp;rsquo;s very easy to whip
up a couple UDFs which do exactly that. Today I&amp;rsquo;ll be showcasing two such functions which respectively add and remove
nullability from Drill &lt;code&gt;FLOAT&lt;/code&gt; variables. As usual you should perform the necessary incantations and summoning rituals
for creating a custom function project in Maven (see the section &amp;ldquo;Downloading Maven and starting a new project&amp;rdquo; in &lt;a href=&#34;http://www.dremio.com/blog/writing-a-custom-sql-function-for-sentiment-analysis/&#34;&gt;this
article&lt;/a&gt; for a refresher).&lt;/p&gt;

&lt;p&gt;Once you&amp;rsquo;re ready to start thinking about source code, the class associated with the function to add nullability looks
like:&lt;/p&gt;

&lt;pre&gt;&lt;code class=&#34;language-java&#34;&gt;package com.yourgroupidentifier.udf;

import org.apache.drill.exec.expr.DrillSimpleFunc;
import org.apache.drill.exec.expr.holders.NullableFloat8Holder;
import org.apache.drill.exec.expr.holders.Float8Holder;

import org.apache.drill.exec.expr.annotations.FunctionTemplate;
import org.apache.drill.exec.expr.annotations.Output;
import org.apache.drill.exec.expr.annotations.Param;

@FunctionTemplate(
        name = &amp;quot;add_null_float&amp;quot;,
        scope = FunctionTemplate.FunctionScope.SIMPLE,
        nulls = FunctionTemplate.NullHandling.INTERNAL
)

public class addNullFloat implements DrillSimpleFunc {

    @Param
    Float8Holder input;

    @Output
    NullableFloat8Holder output;

    public void setup() {
    }

    public void eval() {
        output.isSet = 1;
        output.value = input.value;
    }
}
&lt;/code&gt;&lt;/pre&gt;

&lt;p&gt;while the one for removing nullability is as follows:&lt;/p&gt;

&lt;pre&gt;&lt;code class=&#34;language-java&#34;&gt;package com.yourgroupidentifier.udf;

import org.apache.drill.exec.expr.DrillSimpleFunc;
import org.apache.drill.exec.expr.holders.NullableFloat8Holder;
import org.apache.drill.exec.expr.holders.Float8Holder;

import org.apache.drill.exec.expr.annotations.FunctionTemplate;
import org.apache.drill.exec.expr.annotations.Output;
import org.apache.drill.exec.expr.annotations.Param;

@FunctionTemplate(
        name = &amp;quot;remove_null_float&amp;quot;,
        scope = FunctionTemplate.FunctionScope.SIMPLE,
        nulls = FunctionTemplate.NullHandling.INTERNAL
)

public class removeNullFloat implements DrillSimpleFunc {

    @Param
    NullableFloat8Holder input;

    @Output
    Float8Holder output;

    public void setup() {
    }

    public void eval() {
        output.value = input.value;
    }
}
&lt;/code&gt;&lt;/pre&gt;

&lt;p&gt;Remember back in &lt;a href=&#34;http://www.dremio.com/blog/writing-a-custom-sql-function-for-sentiment-analysis/&#34;&gt;this post&lt;/a&gt; when I
thought I was presenting the simplest UDFs I&amp;rsquo;d ever seen demonstrated? Well, these guys are definitely setting a new
world record. They&amp;rsquo;re just about the smallest custom functions you can code for Drill. But that doesn&amp;rsquo;t mean they&amp;rsquo;re not
useful! &lt;code&gt;ADD_NULL_FLOAT()&lt;/code&gt; and &lt;code&gt;REMOVE_NULL_FLOAT()&lt;/code&gt; make valuable additions to a Drill power user&amp;rsquo;s toolbox, and I&amp;rsquo;ll
be putting one of them to work in a post later this week.&lt;/p&gt;
</description>
    </item>
    
    <item>
      <title>Smartest and dumbest subreddits as judged by submission title readability</title>
      <link>localhost:1313</link>
      <pubDate>Sat, 06 Feb 2016 00:00:00 +0000</pubDate>
      
      <guid>localhost:1313</guid>
      <description>&lt;p&gt;Alright, time to put the readability UDF from &lt;a href=&#34;http://www.dremio.com/blog/querying-for-reading-level-with-a-simple-udf/&#34;&gt;my last
post&lt;/a&gt; to work on some data! For today&amp;rsquo;s
analysis, I&amp;rsquo;ll once again use this &lt;a href=&#34;https://www.reddit.com/r/datasets/comments/3mg812/full_reddit_submission_corpus_now_available_2006/&#34;&gt;Reddit submission
corpus&lt;/a&gt;, which
contains submission data from from the years 2006-2015.&lt;/p&gt;

&lt;p&gt;The questions that motivate today&amp;rsquo;s analysis are simple, but fun: Which popular subreddits have the highest average
submission title reading level? Which ones have the lowest? Or, more glibly, &amp;ldquo;Which subreddits are smart, and which are
dumb?&amp;rdquo; My custom &lt;code&gt;READABILITY()&lt;/code&gt; function for Drill will help us settle this.&lt;/p&gt;

&lt;p&gt;As usual when performing a slightly sophisticated analysis, I first shuffle the data through some VIEWs in order to
grapple with it on my terms. In this case the prep work consisted of two VIEWs, which were constructed as follows:&lt;/p&gt;

&lt;pre&gt;&lt;code&gt;CREATE VIEW reddit_readability AS
     SELECT title, READABILITY(title) ARI, subreddit
       FROM hdfs.`/data/RS_full_corpus.json`
      WHERE over_18 = &#39;false&#39;;
&lt;/code&gt;&lt;/pre&gt;

&lt;p&gt;which is in turn fed into:&lt;/p&gt;

&lt;pre&gt;&lt;code&gt;CREATE VIEW reddit AS
     SELECT subreddit, COUNT(title) posts, AVG(ARI) `avg ARI`
       FROM reddit_readability
      GROUP BY subreddit;
&lt;/code&gt;&lt;/pre&gt;

&lt;p&gt;From here the query to ask for &amp;lsquo;smart&amp;rsquo; subreddits (as I&amp;rsquo;ve defined them) is easy:&lt;/p&gt;

&lt;pre&gt;&lt;code&gt;&amp;gt; SELECT subreddit, posts, `avg ARI` FROM reddit WHERE posts &amp;gt; 100000 ORDER BY `avg ARI` DESC LIMIT 10;
+------------------------+---------+---------------------+
|       subreddit        |  posts  |       avg ARI       |
+------------------------+---------+---------------------+
| spam                   | 287305  | 15.658034632014635  |
| longtail               | 134202  | 15.38567146310872   |
| ModerationLog          | 356311  | 12.622485326455028  |
| modlog                 | 266188  | 12.477493297559754  |
| RisingThreads          | 142485  | 11.752936255500762  |
| worldpolitics          | 163290  | 11.633828401867433  |
| WritingPrompts         | 150776  | 11.348865875111446  |
| environment            | 185858  | 11.167274719418218  |
| Random_Acts_Of_Amazon  | 184420  | 11.14375363175314   |
| conspiro               | 179705  | 10.868244970017907  |
+------------------------+---------+---------------------+
&lt;/code&gt;&lt;/pre&gt;

&lt;p&gt;These results aren&amp;rsquo;t too surprising. Once the internal Reddit stuff is out of the way you&amp;rsquo;re left with some subjects
that are pretty stereotypically high-minded: global politics, literary pursuits, and environmental issues. Also
conspiracy theories, for some reason. That one&amp;rsquo;s weird.&lt;/p&gt;

&lt;p&gt;Alright! On to the dumb stuff!&lt;/p&gt;

&lt;pre&gt;&lt;code&gt;&amp;gt; SELECT subreddit, posts, `avg ARI` FROM reddit WHERE posts &amp;gt; 100000 ORDER BY `avg ARI` LIMIT 10;
+-----------------------+----------+---------------------+
|       subreddit       |  posts   |       avg ARI       |
+-----------------------+----------+---------------------+
| me_irl                | 132477   | -6.597215524826628  |
| itookapicture         | 226074   | 2.591935687626967   |
| Fireteams             | 1600719  | 3.1183569978499373  |
| amiugly               | 108643   | 3.135287171164027   |
| Kikpals               | 170943   | 3.3521135496701313  |
| offmychest            | 242639   | 3.794396824263522   |
| Jokes                 | 163818   | 4.192728884240217   |
| 4chan                 | 107196   | 4.337776062292176   |
| GlobalOffensiveTrade  | 823488   | 4.346750167026149   |
| reactiongifs          | 279156   | 4.464272224403798   |
+-----------------------+----------+---------------------+
&lt;/code&gt;&lt;/pre&gt;

&lt;p&gt;Yup. I can see why these subreddits are dumb. They might not be &lt;em&gt;bad&lt;/em&gt;, but they&amp;rsquo;re definitely not exactly intellectual. Instead it looks like they focus on funny stuff (jokes, Internet memes, gifs) and personal vanity.&lt;/p&gt;

&lt;p&gt;I&amp;rsquo;d like to close on a more technical note that may help those of you looking to perform similar analyses: If you want
to use a Drill UDF in a cluster setting, as I did here (a six-node HDFS configuration, for those curious) be sure to
copy the relevant .jar files to the &lt;code&gt;jars/3rdparty&lt;/code&gt; directory of each machine&amp;rsquo;s Drill install. That&amp;rsquo;s all the setup required
to start using the custom functions you&amp;rsquo;ve written on big data right away!&lt;/p&gt;
</description>
    </item>
    
    <item>
      <title>Querying for reading level with a simple UDF</title>
      <link>localhost:1313</link>
      <pubDate>Fri, 05 Feb 2016 00:00:00 +0000</pubDate>
      
      <guid>localhost:1313</guid>
      <description>&lt;script type=&#34;text/javascript&#34;
 src=&#34;http://cdn.mathjax.org/mathjax/latest/MathJax.js?config=TeX-AMS-MML_HTMLorMML&#34;&gt;
&lt;/script&gt;

&lt;p&gt;Today, just like in &lt;a href=&#34;http://www.dremio.com/blog/writing-a-custom-sql-function-for-sentiment-analysis/&#34;&gt;this post&lt;/a&gt; from
the previous week, I&amp;rsquo;d like to discuss creating a simple custom SQL function for Drill that maps strings to float
values. Except this week&amp;rsquo;s function is even &lt;em&gt;more&lt;/em&gt; simple because it can fit within a single file and requires no
instructions in the &lt;code&gt;setup()&lt;/code&gt; method. In fact, this may be the simplest example of a Drill UDF I&amp;rsquo;ve ever seen, so if
you&amp;rsquo;ve been struggling with how to go about writing your own, the source code I&amp;rsquo;m presenting today maybe a good way to
get some traction.&lt;/p&gt;

&lt;p&gt;The raison d&amp;rsquo;&amp;ecirc;tre of today &amp;rsquo;s function is to calculate the reading level (or, &amp;lsquo;readability&amp;rsquo;) of a single sentence.
Many solutions to the problem of readability utilize syllable counts, which are notoriously difficult to arrive at
computationally. It&amp;rsquo;s possible that a lookup table for those counts would provide satisfactorily speedy results, but the
algorithm that I&amp;rsquo;ve chosen to implement, called the automated readability index or ARI, avoids this problem by
altogether using a character count instead. As per the &lt;a href=&#34;https://en.wikipedia.org/wiki/Automated_readability_index&#34;&gt;Wikipedia
article&lt;/a&gt;, the ARI is arrived at via:&lt;/p&gt;

&lt;p&gt;$$ ARI = 4.71 \frac{characters}{words} + 0.5 \frac{words}{sentences} - 21.43 $$&lt;/p&gt;

&lt;p&gt;However, as I indicated earlier I&amp;rsquo;m only interested in the readability of single sentences in this particular
application (check out the next article!), so I&amp;rsquo;m going to implicitly set the number of sentences to 1 in the source code
that comes later.&lt;/p&gt;

&lt;p&gt;But before I talk about source you should probably first get some UDF-creation boilerplate out of the way.  I&amp;rsquo;ve
discussed how to do this a couple times, but if you&amp;rsquo;re still unsure of what to do go ahead and follow the instructions
in the &amp;ldquo;Downloading Maven and starting a new project&amp;rdquo; section near the beginning of &lt;a href=&#34;http://www.dremio.com/blog/writing-a-custom-sql-function-for-sentiment-analysis/&#34;&gt;this
article&lt;/a&gt;.&lt;/p&gt;

&lt;p&gt;Once that&amp;rsquo;s out of the way, place this single file (&lt;code&gt;Readability.java&lt;/code&gt;) in your project&amp;rsquo;s
&lt;code&gt;main/java/com/yourgroupidentifier/udf&lt;/code&gt; directory:&lt;/p&gt;

&lt;pre&gt;&lt;code class=&#34;language-java&#34;&gt;package com.yourgroupidentifier.udf;

import org.apache.drill.exec.expr.DrillSimpleFunc;
import org.apache.drill.exec.expr.holders.NullableFloat8Holder;
import org.apache.drill.exec.expr.holders.NullableVarCharHolder;

import org.apache.drill.exec.expr.annotations.FunctionTemplate;
import org.apache.drill.exec.expr.annotations.Output;
import org.apache.drill.exec.expr.annotations.Param;

@FunctionTemplate(
        name = &amp;quot;readability&amp;quot;,
        scope = FunctionTemplate.FunctionScope.SIMPLE,
        nulls = FunctionTemplate.NullHandling.NULL_IF_NULL
)

public class Readability implements DrillSimpleFunc {

    @Param
    NullableVarCharHolder input;

    @Output
    NullableFloat8Holder out;

    public void setup() {
    }

    public void eval() {

        // The length of &#39;pneumonoultramicroscopicsilicovolcanoconiosis&#39;
        final int longestWord = 45;

        // Initialize output value
        out.value = 0.0;

        // Split input string up into words
        String inputString = org.apache.drill.exec.expr.fn.impl.StringFunctionHelpers.toStringFromUTF8(input.start,
input.end, input.buffer);
        String[] inputStringWords = inputString.split(&amp;quot;\\s+&amp;quot;);

        float numWords = inputStringWords.length;
        float numCharacters = inputString.length() - (numWords-1); // Accounts for spaces

        // Adjust for things in the text that aren&#39;t words
        // i.e., They are longer than &#39;longestWord&#39;
        for(int i = 0; i &amp;lt; inputStringWords.length; i++) {
            if( inputStringWords[i].length() &amp;gt; longestWord) {
                numWords--;
                numCharacters = numCharacters - inputStringWords[i].length();
            }
        }

        // Output &#39;NULL&#39; if the number of words is zero
        if(numWords != 0) {
            out.value = 4.71 * (numCharacters / numWords) + 0.5 * (numWords) - 21.43;
        }
        else {
            out.isSet = 0;
        }
    }
}
&lt;/code&gt;&lt;/pre&gt;

&lt;p&gt;This is pretty straight forward compared to the &lt;a href=&#34;http://www.dremio.com/blog/writing-a-custom-sql-function-for-sentiment-analysis/&#34;&gt;other
examples&lt;/a&gt; I&amp;rsquo;ve &lt;a href=&#34;http://www.dremio.com/blog/querying-google-analytics-json-with-a-custom-sql-function/&#34;&gt;discussed
before&lt;/a&gt;, right? Just about the
only &amp;lsquo;trick&amp;rsquo; here is that I&amp;rsquo;ve made the number the function returns a &amp;lsquo;Nullable&amp;rsquo; type. This is to insure that it has a
more sane output than &amp;lsquo;Infinity&amp;rsquo; when it encounters a field with zero words&amp;mdash;especially useful for when the
function is used in conjunction with &lt;code&gt;AVG()&lt;/code&gt;, which disregards NULL values but would propagate any &amp;lsquo;Infinity&amp;rsquo; to the
final result.&lt;/p&gt;

&lt;p&gt;In the next post, we&amp;rsquo;ll try this function out in the &amp;lsquo;field&amp;rsquo; on one of my favorite data sets!&lt;/p&gt;

&lt;p&gt;(And I &lt;em&gt;swear&lt;/em&gt; I didn&amp;rsquo;t make that pun intentionally.)&lt;/p&gt;
</description>
    </item>
    
    <item>
      <title>The case of the stolen candy hearts: Advanced date parsing in SQL</title>
      <link>localhost:1313</link>
      <pubDate>Tue, 02 Feb 2016 00:00:00 +0000</pubDate>
      
      <guid>localhost:1313</guid>
      <description>&lt;p&gt;The other day I had 12 years of &lt;a href=&#34;https://data.sfgov.org/Public-Safety/Map-Crime-Incidents-from-1-Jan-2003/gxxq-x39z&#34;&gt;San Francisco crime
data&lt;/a&gt; loaded in Drill and I wanted
to answer the following question: Which days from recent years have the highest incidences of crime?&lt;/p&gt;

&lt;p&gt;As it turns out, this isn&amp;rsquo;t that difficult to accomplish, but it did add some new functions to my repertoire, so I
thought I&amp;rsquo;d share the process with you.&lt;/p&gt;

&lt;p&gt;Once I got a hold of the SF crime download, I renamed it to a file with a &amp;lsquo;.csvh&amp;rsquo; extension so I could address the data
by the column name given in the header. And as we can see in this simple query of the data&lt;/p&gt;

&lt;pre&gt;&lt;code&gt;&amp;gt; SELECT Category, `Date`, `Time`, Address FROM dfs.`/path/to/sfcrime.csvh` LIMIT 5;
+------------------------------+-------------+--------+---------------------------+
|           Category           |    Date     |  Time  |          Address          |
+------------------------------+-------------+--------+---------------------------+
| VANDALISM                    | 01/14/2016  | 23:45  | 3600 Block of ALEMANY BL  |
| ASSAULT                      | 01/14/2016  | 23:45  | 0 Block of DRUMM ST       |
| OTHER OFFENSES               | 01/14/2016  | 23:29  | PALOU AV / LANE ST        |
| DRIVING UNDER THE INFLUENCE  | 01/14/2016  | 23:29  | PALOU AV / LANE ST        |
| OTHER OFFENSES               | 01/14/2016  | 23:00  | 100 Block of DAKOTA ST    |
+------------------------------+-------------+--------+---------------------------+
&lt;/code&gt;&lt;/pre&gt;

&lt;p&gt;the column labeled &amp;lsquo;Date&amp;rsquo; follows the &amp;lsquo;MM/dd/yyy&amp;rsquo; format, so I&amp;rsquo;ll want to keep that in mind when I use the &lt;code&gt;TO_DATE()&lt;/code&gt;
function to transform that entry from a string to a &lt;code&gt;DATE&lt;/code&gt;.&lt;/p&gt;

&lt;p&gt;But remember, the ultimate goal is to find out which days of a given year have the highest crime. To do this I&amp;rsquo;ll need
to make use of the &lt;code&gt;EXTRACT()&lt;/code&gt; function to pull the month, day, and year number from my newly constructed &lt;code&gt;DATE&lt;/code&gt; types.
This is the function that was new to me, but thankfully it&amp;rsquo;s very easy to understand. You just specify which component
of the date you&amp;rsquo;d like to pull as part of the argument, as in:&lt;/p&gt;

&lt;pre&gt;&lt;code&gt;EXTRACT(day FROM myDate)
&lt;/code&gt;&lt;/pre&gt;

&lt;p&gt;I ended up converting the &amp;lsquo;Date&amp;rsquo; column and performing the necessary EXTRACTs at the same time in this view (remember to
&lt;code&gt;USE dfs.tmp;&lt;/code&gt; before entering this command):&lt;/p&gt;

&lt;pre&gt;&lt;code&gt;CREATE VIEW crimedays AS
     SELECT EXTRACT(month FROM TO_DATE(`Date`,&#39;MM/dd/yyyy&#39;)) month_num, EXTRACT(day FROM TO_DATE(`Date`,&#39;MM/dd/yyyy&#39;)) day_num,
            EXTRACT(year FROM TO_DATE(`Date`,&#39;MM/dd/yyyy&#39;)) year_num, IncidntNum id, Category type
       FROM dfs.`/path/to/sfcrime.csvh`;
&lt;/code&gt;&lt;/pre&gt;

&lt;p&gt;So now all it takes is a query with two GROUP BYs on the month and day number to come up with a list of high crime days
for a previous year. Let&amp;rsquo;s take a look at 2014:&lt;/p&gt;

&lt;pre&gt;&lt;code&gt;  SELECT month_num, day_num, year_num, COUNT(id) crimes
    FROM crimedays
   WHERE year_num = 2014
GROUP BY month_num, day_num, year_num
ORDER BY crimes DESC
   LIMIT 5;
+------------+----------+-----------+---------+
| month_num  | day_num  | year_num  | crimes  |
+------------+----------+-----------+---------+
| 10         | 11       | 2014      | 521     |
| 2          | 14       | 2014      | 514     |
| 3          | 19       | 2014      | 513     |
| 8          | 8        | 2014      | 511     |
| 8          | 9        | 2014      | 509     |
+------------+----------+-----------+---------+
&lt;/code&gt;&lt;/pre&gt;

&lt;p&gt;Apparently February 14th was an especially high crime day that year. So this coming Valentine&amp;rsquo;s Day, don&amp;rsquo;t forget to buy
your significant other something nice. But also maybe take some extra care making sure no one steals it before you can
give it to them!&lt;/p&gt;
</description>
    </item>
    
    <item>
      <title>Reddit hates George Bush more than Vladimir Putin</title>
      <link>localhost:1313</link>
      <pubDate>Fri, 29 Jan 2016 00:00:00 +0000</pubDate>
      
      <guid>localhost:1313</guid>
      <description>&lt;p&gt;Just as I promised, today I&amp;rsquo;m going to show off that nifty sentiment analysis UDF for Apache Drill that I discussed in
&lt;a href=&#34;http://www.dremio.com/blog/writing-a-custom-sql-function-for-sentiment-analysis/&#34;&gt;the last article&lt;/a&gt;.&lt;/p&gt;

&lt;p&gt;Today&amp;rsquo;s data is &lt;a href=&#34;http://www.dremio.com/blog/old-and-busted-teasing-formerly-fashionable-websites-from-reddit-data/&#34;&gt;once
again&lt;/a&gt; provided by
this &lt;a href=&#34;https://www.reddit.com/r/datasets/comments/3mg812/full_reddit_submission_corpus_now_available_2006/&#34;&gt;awesome dump of Reddit
submissions&lt;/a&gt; that
date from 2006 up through last summer. Basically I just ran the sentiment analyzer function through submission titles,
examining a selection of politicians that I thought Reddit might feel strongly about.&lt;/p&gt;

&lt;p&gt;In terms of nitty-gritty Drill stuff, I started by first making a view for the data that includes the sentiment score as
computed by the star of yesterday&amp;rsquo;s post, the &lt;code&gt;SIMPLESENT()&lt;/code&gt; function:&lt;/p&gt;

&lt;pre&gt;&lt;code&gt;&amp;gt; USE dfs.tmp;
&amp;gt; CREATE VIEW testview AS SELECT LOWER(title) title, TO_TIMESTAMP(CAST(created_utc AS INT)) created, score, SIMPLESENT(title) sentiment FROM hdfs.`/data/RS_full_corpus.json`;
&lt;/code&gt;&lt;/pre&gt;

&lt;p&gt;And from there I just computed a simple average of those scores over the entire corpus:&lt;/p&gt;

&lt;pre&gt;&lt;code&gt;&amp;gt; SELECT AVG(sentiment) FROM testview WHERE title LIKE &#39;%donald trump%&#39;;
+------------------------+
|         EXPR$0         |
+------------------------+
| -0.052544239386344636  |
+------------------------+
&lt;/code&gt;&lt;/pre&gt;

&lt;p&gt;Remember negative scores indicate negative feelings, and likewise for positive scores. The somewhat surprising results
are compiled below in Figure 1.&lt;/p&gt;

&lt;p&gt;&lt;p style=&#34;text-align: center;&#34;&gt;
&lt;img style=&#34;max-width: 100%;&#34; src=&#34;localhost:1313/img/reddit_politicians.png&#34;&gt;
&lt;/p&gt;
&lt;p style=&#34;text-align: center; font-style: italic;&#34;&gt;&lt;b&gt;Figure 1&lt;/b&gt;: Sentiment analysis for various politicians based on
Reddit submission title.&lt;/p&gt;
&lt;br&gt;
&lt;br&gt;
&lt;br&gt;
&lt;br&gt;&lt;/p&gt;

&lt;p&gt;So, yes, according to this sentiment analysis, Reddit definitely dislikes George Bush more than Vladimir Putin. I always
think of Reddit as overall leaning a bit left in terms of politics, so it didn&amp;rsquo;t shock me to see Barack Obama and Bernie
Sanders show up with positive values. However, it &lt;em&gt;did&lt;/em&gt; surprise me to see Hillary Clinton score negatively. And not
only that, she scored even &lt;em&gt;more&lt;/em&gt; negatively than Sarah Palin!&lt;/p&gt;

&lt;p&gt;Finally, those of us who were frequent redditors during the 2008 election season will be far from confused by the
average sentiment score achieved by then nominal-Republican Ron Paul.&lt;/p&gt;

&lt;p&gt;Reddit loves that guy.&lt;/p&gt;
</description>
    </item>
    
    <item>
      <title>Writing a custom SQL function for sentiment analysis</title>
      <link>localhost:1313</link>
      <pubDate>Thu, 28 Jan 2016 00:00:00 +0000</pubDate>
      
      <guid>localhost:1313</guid>
      <description>

&lt;p&gt;In the world of data analytics a &amp;lsquo;sentiment analysis&amp;rsquo; is any technique that attempts to represent the feelings of users
in a somewhat quantitative way. Implementations of this idea vary, but one of the simplest ones involves giving
individual words a numeric score according to the strength of the positive or negative the emotions that they elicit.
For instance we might assign a score of -2.3 to the word &amp;lsquo;disappointment&amp;rsquo; and a score of 1.8 to &amp;lsquo;lighthearted.&amp;rsquo;&lt;/p&gt;

&lt;p&gt;In today&amp;rsquo;s article I&amp;rsquo;m going to demonstrate that writing a custom SQL function (also known as a user defined function,
or UDF) that performs a sentiment analysis is a fairly straightforward task. The SQL platform we&amp;rsquo;ll be using for this
project is Apache Drill; A software capable of querying &lt;em&gt;many&lt;/em&gt; different types of data stores that also allows for the
creation of custom functions written in the Java programming language.&lt;/p&gt;

&lt;p&gt;This isn&amp;rsquo;t the first time I&amp;rsquo;ve written about creating a UDF for Drill, and readers looking for a richer set of
information about UDF programming may want to refer to &lt;a href=&#34;http://www.dremio.com/blog/querying-google-analytics-json-with-a-custom-sql-function/&#34;&gt;this earlier
article&lt;/a&gt;.&lt;/p&gt;

&lt;h2 id=&#34;downloading-maven-and-starting-a-new-project&#34;&gt;Downloading Maven and starting a new project&lt;/h2&gt;

&lt;p&gt;Just as before, we&amp;rsquo;ll want to start by downloading and installing Apache Maven (&lt;a href=&#34;https://maven.apache.org/download.cgi&#34;&gt;available
here&lt;/a&gt;), which will be responsible for managing and building our Java project:&lt;/p&gt;

&lt;pre&gt;&lt;code&gt;$ tar xzvf apache-maven-3.3.9-bin.tar.gz
$ mv apache-maven-3.3.9 apache-maven
&lt;/code&gt;&lt;/pre&gt;

&lt;p&gt;And since you&amp;rsquo;ll probably be using it fairly frequently it might be nice to put the Maven binary in the PATH environment
variable, so add this line to your &lt;code&gt;.bashrc&lt;/code&gt; file:&lt;/p&gt;

&lt;pre&gt;&lt;code&gt;export PATH=$PATH:~/apache-maven/bin
&lt;/code&gt;&lt;/pre&gt;

&lt;p&gt;Now go to whatever directory you&amp;rsquo;d like to store your UDFs in and issue this Maven command to create a new project for
our sentiment analyzer called &amp;lsquo;simplesentiment&amp;rsquo;:&lt;/p&gt;

&lt;pre&gt;&lt;code&gt;$ mvn archetype:generate -DgroupId=com.dremio.app -DartifactId=simplesentiment -DinteractiveMode=false
&lt;/code&gt;&lt;/pre&gt;

&lt;p&gt;Because our UDF relies on Apache Drill, we&amp;rsquo;ll need to add a couple things to the project&amp;rsquo;s &lt;code&gt;pom.xml&lt;/code&gt; configuration file.
The first should go within the &lt;code&gt;&amp;lt;dependencies&amp;gt;&lt;/code&gt; tag:&lt;/p&gt;

&lt;pre&gt;&lt;code&gt;&amp;lt;dependency&amp;gt;
  &amp;lt;groupId&amp;gt;org.apache.drill.exec&amp;lt;/groupId&amp;gt;
  &amp;lt;artifactId&amp;gt;drill-java-exec&amp;lt;/artifactId&amp;gt;
  &amp;lt;version&amp;gt;1.4.0&amp;lt;/version&amp;gt;
&amp;lt;/dependency&amp;gt;
&lt;/code&gt;&lt;/pre&gt;

&lt;p&gt;and the next should go inside the outermost tag called &lt;code&gt;&amp;lt;project&amp;gt;&lt;/code&gt;:&lt;/p&gt;

&lt;pre&gt;&lt;code&gt;&amp;lt;build&amp;gt;
  &amp;lt;plugins&amp;gt;
    &amp;lt;plugin&amp;gt;
      &amp;lt;groupId&amp;gt;org.apache.maven.plugins&amp;lt;/groupId&amp;gt;
      &amp;lt;artifactId&amp;gt;maven-source-plugin&amp;lt;/artifactId&amp;gt;
      &amp;lt;version&amp;gt;2.4&amp;lt;/version&amp;gt;
      &amp;lt;executions&amp;gt;
        &amp;lt;execution&amp;gt;
          &amp;lt;id&amp;gt;attach-sources&amp;lt;/id&amp;gt;
          &amp;lt;phase&amp;gt;package&amp;lt;/phase&amp;gt;
          &amp;lt;goals&amp;gt;
            &amp;lt;goal&amp;gt;jar-no-fork&amp;lt;/goal&amp;gt;
          &amp;lt;/goals&amp;gt;
        &amp;lt;/execution&amp;gt;
      &amp;lt;/executions&amp;gt;
    &amp;lt;/plugin&amp;gt;
  &amp;lt;/plugins&amp;gt;
&amp;lt;/build&amp;gt;
&lt;/code&gt;&lt;/pre&gt;

&lt;p&gt;Finally we need to make a &lt;code&gt;./src/main/resources/drill-module.conf&lt;/code&gt; file for the project (you&amp;rsquo;ll probably need to create
the &amp;lsquo;resources&amp;rsquo; directory, so go ahead and do that). This file should have these contents:&lt;/p&gt;

&lt;pre&gt;&lt;code&gt;drill {
  classpath.scanning {
    packages : ${?drill.classpath.scanning.packages} [
      com.yourgroupidentifier.udf
    ]
  }
}
&lt;/code&gt;&lt;/pre&gt;

&lt;p&gt;Where &lt;code&gt;com.yourgroupidentifier.udf&lt;/code&gt; should be the same name as the &lt;code&gt;package&lt;/code&gt; specified in the Java files listed in the
next section.&lt;/p&gt;

&lt;h2 id=&#34;sentiment-analysis-udf-source-code&#34;&gt;Sentiment analysis UDF source code&lt;/h2&gt;

&lt;p&gt;The sentiment analyzer in my UDF follows the simple algorithm that I described earlier, with the values for words
provided by &lt;a href=&#34;https://github.com/cjhutto/vaderSentiment/blob/master/build/lib/vaderSentiment/vader_sentiment_lexicon.txt&#34;&gt;this
file&lt;/a&gt;
(&lt;code&gt;vader_sentiment_lexicon.txt&lt;/code&gt;) available on Github from user &amp;lsquo;cjhutto.&amp;rsquo;&lt;/p&gt;

&lt;p&gt;Because Drill is picky about the format of a UDF class, this custom function had to be expressed in two different source
files: one to define all the function&amp;rsquo;s operations, and another for a simple class to hold the dictionary that
translates words to numeric sentiment values. For this project, these files will be located in the project&amp;rsquo;s
&lt;code&gt;main/java/com/yourgroupidentifier/udf&lt;/code&gt; directory.&lt;/p&gt;

&lt;p&gt;The first file, &lt;code&gt;simpleSent.java&lt;/code&gt; looks like:&lt;/p&gt;

&lt;pre&gt;&lt;code class=&#34;language-java&#34;&gt;package com.yourgroupidentifier.udf;

import org.apache.drill.exec.expr.DrillSimpleFunc;
import org.apache.drill.exec.expr.holders.Float8Holder;
import org.apache.drill.exec.expr.holders.NullableVarCharHolder;

import org.apache.drill.exec.expr.annotations.FunctionTemplate;
import org.apache.drill.exec.expr.annotations.Output;
import org.apache.drill.exec.expr.annotations.Param;

@FunctionTemplate(
        name = &amp;quot;simplesent&amp;quot;,
        scope = FunctionTemplate.FunctionScope.SIMPLE,
        nulls = FunctionTemplate.NullHandling.NULL_IF_NULL
)

public class simpleSent implements DrillSimpleFunc {

    // The input to the function---almost certainly a text field
    @Param
    NullableVarCharHolder input;

    // The output of the function---just a number
    @Output
    Float8Holder out;

    public void setup() {

        // Initialize object that holds dictionary
        new com.yourgroupidentifier.udf.dictHolder();

        // Open the sentiment values file
        try {
            java.io.FileReader fileReader = new java.io.FileReader(&amp;quot;/path/to/vader_sentiment_lexicon.txt&amp;quot;);
            java.io.BufferedReader bufferedReader = new java.io.BufferedReader(fileReader);

            String currLine;

            // Read each line
            try {
                while ((currLine = bufferedReader.readLine()) != null) {
                    String[] splitLine = currLine.split(&amp;quot;\\s+&amp;quot;);

                    String currWord = splitLine[0];
                    Double currValue;
                    try {
                        currValue = Double.parseDouble(splitLine[1]);
                    }
                    catch (java.lang.NumberFormatException numberEx) {
                        currValue = 0.0;
                    }

                    // Put sentiment value in dictionary
                    com.yourgroupidentifier.udf.dictHolder.sentiDict.put(currWord, currValue);
                }
            }
            catch(java.io.IOException ioEx) {
                System.out.print(&amp;quot;IOException encountered&amp;quot;);
            }

        }
        catch(java.io.FileNotFoundException fileEx) {
            System.out.println(&amp;quot;Sentiment valences file not found!&amp;quot;);
        }
    }

    public void eval() {

        // Initialize output value
        out.value = 0.0;

        // Split up the input string
        String inputString = org.apache.drill.exec.expr.fn.impl.StringFunctionHelpers.toStringFromUTF8(input.start, input.end, input.buffer);
        String[] splitInputString = inputString.split(&amp;quot;\\s+&amp;quot;);

        for(int i = 0; i &amp;lt; splitInputString.length; i++) {

            java.lang.Object result = com.yourgroupidentifier.udf.dictHolder.sentiDict.get(splitInputString[i].toLowerCase());

            if(result != null) {

                Double wordValue = ((Double) result);

                out.value += wordValue;
            }
        }

    }

}
&lt;/code&gt;&lt;/pre&gt;

&lt;p&gt;(Remember to change the line with &lt;code&gt;/path/to/vader_sentiment_lexicon.txt&lt;/code&gt; so that it reflects the location of the file on
your system!)&lt;/p&gt;

&lt;p&gt;The second file is called &lt;code&gt;dictHolder.java&lt;/code&gt;, and contains this small class:&lt;/p&gt;

&lt;pre&gt;&lt;code class=&#34;language-java&#34;&gt;package com.yourgroupidentifier.udf;

public class dictHolder {
    static public java.util.Hashtable&amp;lt;String, Double&amp;gt; sentiDict;

    public dictHolder() {
        sentiDict = new java.util.Hashtable&amp;lt;String, Double&amp;gt;();
    }
}
&lt;/code&gt;&lt;/pre&gt;

&lt;h2 id=&#34;building-and-installing-our-udf&#34;&gt;Building and installing our UDF&lt;/h2&gt;

&lt;p&gt;To build and install the custom function, just go to the project&amp;rsquo;s root directory (the one with &lt;code&gt;pom.xml&lt;/code&gt;) and issue
these commands&lt;/p&gt;

&lt;pre&gt;&lt;code&gt;$ mvn clean package
$ cp target/*.jar ~/apache-drill/jars/3rdparty
&lt;/code&gt;&lt;/pre&gt;

&lt;p&gt;changing the second command to be appropriate for your Drill install.&lt;/p&gt;

&lt;p&gt;And that&amp;rsquo;s literally all there is to it! You should now be able to invoke the &lt;code&gt;SIMPLESENT()&lt;/code&gt; function from within
Drill&amp;rsquo;s SQL prompt. In the next article I&amp;rsquo;ll be doing exactly that as I explore a corpus of Reddit submission titles
using this handy new analysis tool.&lt;/p&gt;
</description>
    </item>
    
    <item>
      <title>Securing SQL on Hadoop, Part 2: Installing and configuring Drill</title>
      <link>localhost:1313</link>
      <pubDate>Mon, 25 Jan 2016 00:00:00 +0000</pubDate>
      
      <guid>localhost:1313</guid>
      <description>

&lt;p&gt;Today we&amp;rsquo;re going to pick up where we left off in &lt;a href=&#34;http://www.dremio.com/blog/securing-sql-on-hadoop-part-1-installing-cdh-and-kerberos/&#34;&gt;Part
1&lt;/a&gt; of my two-parter about setting
up a CDH cluster to perform secure SQL queries on an HDFS store. As you recall, last time we had just finished using
Cloudera Manager&amp;rsquo;s wizard to finalize a Kerberos configuration, and all of the cluster services had come back online
using the new security system so our basic HDFS cluster set-up was good to go. All that&amp;rsquo;s left to do now is install and
configure the piece of software that implements the SQL interface: Apache Drill.&lt;/p&gt;

&lt;h2 id=&#34;step-1-drill-prerequisites&#34;&gt;Step 1.) Drill prerequisites&lt;/h2&gt;

&lt;p&gt;First things first: We&amp;rsquo;re gonna need Java 7 or better on our CDH machines. Following some useful information found
&lt;a href=&#34;http://lifeonubuntu.com/ubuntu-missing-add-apt-repository-command/&#34;&gt;here&lt;/a&gt; and
&lt;a href=&#34;http://askubuntu.com/questions/508546/howto-upgrade-java-on-ubuntu-14-04-lts&#34;&gt;here&lt;/a&gt;, we can set up a new package
repository and pull it down using:&lt;/p&gt;

&lt;pre&gt;&lt;code&gt;$ sudo apt-get install software-properties-common python-software-properties
$ sudo add-apt-repository ppa:webupd8team/java
$ sudo apt-get update
$ sudo apt-get install oracle-java7-installer
&lt;/code&gt;&lt;/pre&gt;

&lt;p&gt;(Remember to do this for each node on your cluster!)&lt;/p&gt;

&lt;h2 id=&#34;step-2-install-drill&#34;&gt;Step 2.) Install Drill&lt;/h2&gt;

&lt;p&gt;Time to install Drill and tell it where the cluster&amp;rsquo;s &amp;lsquo;Zookeeper&amp;rsquo; is (REMEMBER: Just like installing Java, this step
needs to be done on every node). Start by downloading and unpacking the software:&lt;/p&gt;

&lt;pre&gt;&lt;code&gt;$ wget http://apache.osuosl.org/drill/drill-1.4.0/apache-drill-1.4.0.tar.gz
$ tar xzvf apache-drill-1.4.0.tar.gz
&lt;/code&gt;&lt;/pre&gt;

&lt;p&gt;Next edit Drill&amp;rsquo;s &lt;code&gt;conf/drill-override.conf&lt;/code&gt; file so that it looks like:&lt;/p&gt;

&lt;pre&gt;&lt;code&gt;drill.exec: {
  cluster-id: &amp;quot;drill-cdh-cluster&amp;quot;,
  zk.connect: &amp;quot;&amp;lt;ZOOKEEPER IP&amp;gt;:2181&amp;quot;
}
&lt;/code&gt;&lt;/pre&gt;

&lt;p&gt;Where &amp;lsquo;ZOOKEEPER IP&amp;rsquo; should be changed to the IP of the CDH machine that runs the zookeeper process (this is probably
the same as the Main Node from the last article).&lt;/p&gt;

&lt;p&gt;If you&amp;rsquo;d like, you can add Drill to the system path by adding this to &lt;code&gt;.bashrc&lt;/code&gt;:&lt;/p&gt;

&lt;pre&gt;&lt;code&gt;export PATH=$PATH:/apache-drill-1.4.0/bin
&lt;/code&gt;&lt;/pre&gt;

&lt;h2 id=&#34;step-3-configuring-drill-for-hdfs&#34;&gt;Step 3.) Configuring Drill for HDFS&lt;/h2&gt;

&lt;p&gt;Now we need to set up Drill so that it can read our cluster&amp;rsquo;s HDFS. Open the Drill Web Console by going to
http://&amp;lt;MAIN NODE IP&amp;gt;:8047. Click &amp;lsquo;Storage&amp;rsquo; at the top and then &amp;lsquo;Update&amp;rsquo; next to the dfs plugin and copy the JSON
that you find in the text field. Next make a new storage plugin called &amp;lsquo;hdfs&amp;rsquo; (previous page) and then paste in the text
you just copied, replacing the &amp;lsquo;null&amp;rsquo; that&amp;rsquo;s already there.&lt;/p&gt;

&lt;p&gt;We&amp;rsquo;ll proceed by making a small modification that turns this standard &amp;lsquo;dfs&amp;rsquo; plugin in into our new one for HDFS.&lt;/p&gt;

&lt;p&gt;Take the line&lt;/p&gt;

&lt;pre&gt;&lt;code&gt;&amp;quot;connection&amp;quot;: &amp;quot;file:///&amp;quot;,
&lt;/code&gt;&lt;/pre&gt;

&lt;p&gt;and replace it with&lt;/p&gt;

&lt;pre&gt;&lt;code&gt;&amp;quot;connection&amp;quot;: &amp;quot;hdfs://&amp;lt;ADDRESS OF HDFS NAMENODE&amp;gt;:8020&amp;quot;,
&lt;/code&gt;&lt;/pre&gt;

&lt;p&gt;Hit &amp;ldquo;Create&amp;rdquo; and now Drill is ready to read your HDFS data!&lt;/p&gt;

&lt;h2 id=&#34;step-4-enabling-kerberos-support-for-drill&#34;&gt;Step 4.) Enabling Kerberos support for Drill&lt;/h2&gt;

&lt;p&gt;So now we have HDFS, Kerberos, &lt;em&gt;and&lt;/em&gt; Drill, but currently Drill can&amp;rsquo;t talk to the HDFS we have running because it
requires authentication. Let&amp;rsquo;s fix that.&lt;/p&gt;

&lt;p&gt;First we should make an HDFS superuser account as indicated in this &lt;a href=&#34;http://www.cloudera.com/documentation/enterprise/latest/topics/cm_sg_s5_hdfs_principal.html&#34;&gt;Cloudera
doc&lt;/a&gt;. On the Main Node, run
&lt;code&gt;sudo kadmin.local&lt;/code&gt; and add an &amp;lsquo;hdfs&amp;rsquo; principal with this command:&lt;/p&gt;

&lt;pre&gt;&lt;code&gt;addprinc hdfs@KERBEROS.CDH
&lt;/code&gt;&lt;/pre&gt;

&lt;p&gt;(Hit Ctrl-d to exit the prompt). In order to enable authentication with Kerberos, we also need to copy the file
&lt;code&gt;hadoop-yarn-api.jar&lt;/code&gt; into Drill&amp;rsquo;s class path:&lt;/p&gt;

&lt;pre&gt;&lt;code&gt;$ cp /opt/cloudera/parcels/CDH-5.5.1-1.cdh5.5.1.p0.11/lib/hadoop/client/hadoop-yarn-api.jar ~/apache-drill/jars/
&lt;/code&gt;&lt;/pre&gt;

&lt;p&gt;(NOTE: &lt;em&gt;The above step and the three following must be performed on each node of the cluster&lt;/em&gt;.)&lt;/p&gt;

&lt;p&gt;Next, Drill&amp;rsquo;s &lt;code&gt;conf/core-site.xml&lt;/code&gt; file should be edited to contain the following snippet of xml:&lt;/p&gt;

&lt;pre&gt;&lt;code&gt;&amp;lt;property&amp;gt;
  &amp;lt;name&amp;gt;hadoop.security.authentication&amp;lt;/name&amp;gt;
  &amp;lt;value&amp;gt;kerberos&amp;lt;/value&amp;gt;
&amp;lt;/property&amp;gt;
&lt;/code&gt;&lt;/pre&gt;

&lt;p&gt;All that&amp;rsquo;s left to do is create an &amp;lsquo;hdfs&amp;rsquo; Kerberos ticket for the user that we&amp;rsquo;re logged into&lt;/p&gt;

&lt;pre&gt;&lt;code&gt;$ kinit hdfs@KERBEROS.CDH
&lt;/code&gt;&lt;/pre&gt;

&lt;p&gt;and then start up each of the drillbits&lt;/p&gt;

&lt;pre&gt;&lt;code&gt;drillbit.sh start
&lt;/code&gt;&lt;/pre&gt;

&lt;p&gt;So now Drill has both the configuration and the authority to use our kerberized HDFS store. Give it a shot by opening up
a Drill prompt (&lt;code&gt;drill-conf&lt;/code&gt;) and trying a query.&lt;/p&gt;

&lt;p&gt;For example, here&amp;rsquo;s a test query on my handy 250 GB of Reddit data:&lt;/p&gt;

&lt;pre&gt;&lt;code&gt;&amp;gt; SELECT title, TO_TIMESTAMP(CAST(created_utc AS INT)) created, score FROM hdfs.`/data/RS_full_corpus.json` WHERE over_18 = false AND score &amp;gt;= 100 LIMIT 10;
+------------------------------------------------------------------------------+------------------------+--------+
|                                    title                                     |        created         | score  |
+------------------------------------------------------------------------------+------------------------+--------+
| Early Retirement Guide - Phillip Greenspun                                   | 2006-01-30 19:21:17.0  | 223    |
| Programming Like A Mathematician I: Closures                                 | 2006-01-29 18:14:30.0  | 178    |
| More than 1000 wikipedia alterations by US Representative Staffers           | 2006-01-29 12:57:45.0  | 304    |
| Great Design: What is Design?                                                | 2006-01-26 20:54:30.0  | 143    |
| Use Python (not Java) to teach programming                                   | 2006-01-26 19:43:23.0  | 167    |
| The Demotivators!                                                            | 2006-01-26 18:02:51.0  | 168    |
| Just how much can you achieve with pure CSS? Some amazing demonstrations.    | 2006-02-26 18:07:57.0  | 329    |
| A summary of two lectures by Alan Kay                                        | 2006-02-24 04:56:49.0  | 110    |
| How Intel could buy Hollywood and profit by selling more DRM-less machines.  | 2006-02-20 20:48:18.0  | 108    |
| &amp;quot;zeitgeist&amp;quot; reddits, covering popular topics, eg, the infamous cartoons      | 2006-02-19 19:33:59.0  | 299    |
+------------------------------------------------------------------------------+------------------------+--------+
&lt;/code&gt;&lt;/pre&gt;

&lt;p&gt;And that wraps up Part 2 of this how-to guide. May all your queries be fruitful and secure!&lt;/p&gt;

&lt;p&gt;&lt;em&gt;Acknoweldgements: Many thanks to William Witt over on the user@drill.apache.org mailing list for providing crucial
information about Kerberos-Drill configuration!&lt;/em&gt;&lt;/p&gt;
</description>
    </item>
    
  </channel>
</rss>